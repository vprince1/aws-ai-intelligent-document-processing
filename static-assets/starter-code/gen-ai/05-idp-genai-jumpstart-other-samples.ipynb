{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f1110e7-f3c1-4fad-a667-6c4c0eeff469",
   "metadata": {},
   "source": [
    "# Augment Intelligent Document Processing with generative AI\n",
    "---\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> \n",
    "    <b>NOTE:</b> You will need to use a Jupyter Kernel with Python 3.9 or above to use this notebook. If you are in Amazon SageMaker Studio, you can use the \"Data Science 3.0\" image.\n",
    "</div>\n",
    "\n",
    "In this notebook, we demonstrate how you can integrate Amazon Textract with LangChain as a document loader to extract data from documents and use generative AI capabilities within the various IDP phases. We will perform the following with different LLMs.\n",
    "\n",
    "- Classification\n",
    "- Summarization\n",
    "- Standardization\n",
    "- Spell check corrections\n",
    "- Q&A with tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c9a875-2b40-41ae-b3e8-bfef005bcd13",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install langchain huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a37ba1f-5f5f-4a01-b9f7-50fc05e071c7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install amazon-textract-textractor pypdf Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc283ec-201e-4147-9703-b1eaece01886",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "HUGGINGFACEHUB_API_TOKEN = getpass()\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = HUGGINGFACEHUB_API_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671e8f24-f93c-44b2-86da-e4f4f048e3f5",
   "metadata": {},
   "source": [
    "## 1. Classification\n",
    "---\n",
    "\n",
    "Classify a document based on it's content, given a list of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26881c80-0369-4609-9526-40483548d5b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import AmazonTextractPDFLoader\n",
    "from langchain import HuggingFaceHub\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "loader = AmazonTextractPDFLoader(\"./samples/discharge-summary.png\")\n",
    "document = loader.load()\n",
    "\n",
    "template = \"\"\"Given a list of 'Classes', classify the 'Document' into one of these classes. \n",
    "\n",
    "Classes: DISCHARGE_SUMMARY, RECEIPT, PRESCRIPTION\n",
    "Document: {doc_text}\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"doc_text\"])\n",
    "llm = HuggingFaceHub(\n",
    "                repo_id=\"google/flan-t5-xxl\",model_kwargs={\"temperature\": 0.5, \"max_length\": 50}\n",
    ")\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "class_name = llm_chain.run(document[0].page_content)\n",
    "\n",
    "print(f\"The provided document is a {class_name}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a921d1aa-e856-4b72-828f-4b2044f54ba6",
   "metadata": {},
   "source": [
    "## 2. Summarization\n",
    "---\n",
    "\n",
    "Summarize large pieces of text from a document into smaller, more coincise explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4f4de0-f4c7-4488-8b57-03429cddd168",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import AmazonTextractPDFLoader\n",
    "from langchain import HuggingFaceHub\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "loader = AmazonTextractPDFLoader(\"./samples/discharge-summary.png\")\n",
    "document = loader.load()\n",
    "\n",
    "template = \"\"\"Given a full 'Document', summarize it for me. \n",
    "\n",
    "Document: {doc_text}\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"doc_text\"])\n",
    "llm = HuggingFaceHub(\n",
    "                repo_id=\"google/flan-t5-xxl\",model_kwargs={\"temperature\": 0.1, \"max_length\": 512}\n",
    ")\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "summary = llm_chain.run(document[0].page_content)\n",
    "\n",
    "print(f\"Here's the summary of the document\\n\")\n",
    "print(f\"==================================\\n\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81525ec-92fd-41e3-9107-5e6eb67c8070",
   "metadata": {},
   "source": [
    "## Standardization\n",
    "---\n",
    "\n",
    "Note that flan-t5-xxl model has a 1024 token limit. Due to this reason we will divide the problem into two parts\n",
    "\n",
    "- First we ask the model to get the desired value from the document text using prompt template `template1`\n",
    "- Then we get the out put from the first LLM call and pass it on to a second template `template2` for standardization and formatting. `template2` uses few-shot prompting with example to guide the LLM to generate the desired output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b46371f-5e19-481c-afac-a76fff5069f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import AmazonTextractPDFLoader\n",
    "from langchain import HuggingFaceHub\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "loader = AmazonTextractPDFLoader(\"./samples/discharge-summary.png\")\n",
    "document = loader.load()\n",
    "\n",
    "template1 = \"\"\"Given a full 'Document', answer the question. \n",
    "\n",
    "Document: {doc_text}\n",
    "Question: {question}\"\"\"\n",
    "\n",
    "template2 = \"\"\"Convert the dates given in MM/DD/YYYY format.\n",
    "Examples-\n",
    "Date: Nov-14-2023\n",
    "Answer: 11/15/2023\n",
    "Document: 05-Sep-2020\n",
    "Answer: 9/5/2020\n",
    "date: {dt}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "rep = \"google/flan-t5-xxl\"\n",
    "\n",
    "llm = HuggingFaceHub(\n",
    "                repo_id=rep,model_kwargs={\"temperature\": 0.1, \"max_length\": 50}\n",
    ")\n",
    "\n",
    "prompt1 = PromptTemplate(template=template1, input_variables=[\"doc_text\", \"question\"])\n",
    "llm_chain = LLMChain(prompt=prompt1, llm=llm)\n",
    "\n",
    "prompt2 = PromptTemplate(template=template2, input_variables=[\"dt\"])\n",
    "llm_chain2 = LLMChain(prompt=prompt2, llm=llm)\n",
    "\n",
    "chain = ( \n",
    "    llm_chain \n",
    "    | {'dt': lambda x: x['text'] }  \n",
    "    | llm_chain2\n",
    ")\n",
    "\n",
    "std_op = chain.invoke({ \"doc_text\": document[0].page_content, \n",
    "                        \"question\": \"Can you give me the patient admitted date date?\"})\n",
    "\n",
    "print(std_op['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f468d2-cf48-4011-afda-1c602c484bb8",
   "metadata": {},
   "source": [
    "## Spell check and corrections\n",
    "---\n",
    "\n",
    "Perform grammatical and spelling corrections on text extracted from a hand written document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b39346-989b-487c-8a90-f227d8629afd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import AmazonTextractPDFLoader\n",
    "from langchain import HuggingFaceHub\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "loader = AmazonTextractPDFLoader(\"./samples/hand_written_note.pdf\")\n",
    "document = loader.load()\n",
    "\n",
    "\n",
    "template = \"\"\"Given a detailed 'Document', perform spelling and grammatical corrections. Ensure the output is coherent, \n",
    "polished, and free from errors.\n",
    "\n",
    "Document: {doc_text}\n",
    "Corrected text:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"doc_text\"])\n",
    "llm = HuggingFaceHub(\n",
    "                repo_id=\"google/flan-t5-xxl\",model_kwargs={\"temperature\": 0.8, \"max_length\": 1024}\n",
    ")\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "try:\n",
    "    lines = document[0].page_content.split(\".\")\n",
    "    lines = [x.strip(\" \") for x in lines]\n",
    "    lines = list(set(lines))\n",
    "    for line in lines:\n",
    "        if line and line != \" \":\n",
    "            print(\"Extracted text\")\n",
    "            print(\"==============\")\n",
    "            print(line)\n",
    "            std_op = llm_chain.run({\"doc_text\": line})\n",
    "\n",
    "            print(\"Corrected text\")\n",
    "            print(\"==============\")\n",
    "            print(std_op)\n",
    "            print(\"\\n\")\n",
    "except Exception as e:\n",
    "    print(str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e1194a-a304-4781-b513-43fbe8f4e34f",
   "metadata": {},
   "source": [
    "## Q&A with Tables\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a166f72-b189-4327-a389-43f7e687e0b7",
   "metadata": {},
   "source": [
    "If you're on SageMaker Studio environment then you will have to install gcc and gcc-c++ and C++ version 11 compiler. If you're on CentOS then running the following may help if you encounter issues in installing chromaDB.\n",
    "\n",
    "```\n",
    "!apt-get update\n",
    "!apt-get install build-essential -y\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b10b2f-cfc1-46fa-8179-7d447dac15f9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!apt-get update\n",
    "!apt-get install build-essential -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b6abb8-07de-4578-ad44-a8ff1a37b1b4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -U amazon-textract-prettyprinter amazon-textract-textractor langchain spacy -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2deb9727-d852-40b8-a08d-ef9b83d775dc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86d03cb-c3db-485a-864f-c0c43dcdb4de",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -U lark chromadb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3511d10d-c85c-45e5-8fad-fed2a16aabe8",
   "metadata": {},
   "source": [
    "View the list of models available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7620eba-bdfc-4bba-b1e5-585f23e5fa08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# To list all the available textgeneration models in JumpStart uncomment and run the code below\n",
    "from sagemaker.jumpstart.notebook_utils import list_jumpstart_models, list_jumpstart_tasks\n",
    "filter_value = \"task == llm\"\n",
    "\n",
    "print(\"===== Available Models =====\")\n",
    "text_generation_models = list_jumpstart_models(filter=filter_value)\n",
    "text_generation_models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0840dfc-2137-49c8-87eb-16898171571e",
   "metadata": {},
   "source": [
    "### Model selection\n",
    "\n",
    "LangChain's self-querying capabilities need a model that can accept atleast more than 2k token at a time for a reasonably sized table. If you have larger tables you may need larger models. For the purposes of this demonstration we will deploy a Falcon 40b BF16 model. To be able to execute this section you will need access to SageMaker JumpStart models and you must be in us-east-1 region.\n",
    "\n",
    "Note: Using SageMaker JumpStart is just an option of using an LLM, feel free to use any LLM of your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564fde22-bb26-43ac-943c-8fc30e434f19",
   "metadata": {},
   "source": [
    "Please note that deploying this model with SageMaker Jumpstart requires an `ml.g5.12xlarge` instance. Please make sure that you have atleast 1 instance capacity available in the account/region where you are deploying this endpoint. You can check the Quota using the Amazon Service Quota console [here](https://console.aws.amazon.com/servicequotas/home/services/sagemaker/quotas) and search for \"ml.g5.12xlarge\". The \"Applied Quota Value\" must show a value greater than 0.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9137fb28-623c-421d-8bf6-6349d97f23ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "\n",
    "try:\n",
    "    model = JumpStartModel(model_id=model_id, instance_type=\"ml.g5.12xlarge\")\n",
    "    predictor = model.deploy()\n",
    "except Exception as e:\n",
    "    print(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e95133d-f764-4ea7-a2f0-937d32d3d386",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint_name = predictor.endpoint_name\n",
    "region = \"us-east-1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7640358-9371-4e94-b993-182f2a1e4687",
   "metadata": {},
   "source": [
    "## Falcon 40b BF16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385cdf28-af16-4a83-be61-e132c604ece4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(\n",
    "    model_id,\n",
    "    model_version,\n",
    ") = (\n",
    "    \"huggingface-llm-falcon-40b-instruct-bf16\",\n",
    "    \"*\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b68fdea-a061-431f-9efd-946e483151be",
   "metadata": {},
   "source": [
    "Please note that deploying this model with SageMaker Jumpstart requires an `ml.g5.12xlarge` instance. Please make sure that you have atleast 1 instance capacity available in the account/region where you are deploying this endpoint. You can check the Quota using the Amazon Service Quota console [here](https://console.aws.amazon.com/servicequotas/home/services/sagemaker/quotas) and search for \"ml.g5.12xlarge\". The \"Applied Quota Value\" must show a value greater than 0.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e2e799-bc89-4151-b2cb-e8aa3b8d20c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "\n",
    "try:\n",
    "    model = JumpStartModel(model_id=model_id, instance_type=\"ml.g5.12xlarge\")\n",
    "    predictor = model.deploy()\n",
    "except Exception as e:\n",
    "    print(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d3a0d0-69e7-4795-b3e3-5799b33eb0f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint_name = predictor.endpoint_name\n",
    "region = \"us-east-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff871a3-1011-4e29-889b-fd3dc52ddce7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from textractcaller.t_call import call_textract, Textract_Features\n",
    "from trp import Document\n",
    "from trp.trp2 import TDocument, TDocumentSchema\n",
    "from textractprettyprinter.t_pretty_print import get_tables_string, Pretty_Print_Table_Format\n",
    "\n",
    "textract_json = call_textract(input_document=\"./samples/bank_statement.jpg\", features=[Textract_Features.TABLES])\n",
    "\n",
    "doc = Document(textract_json)\n",
    "all_tables = list()\n",
    "for page in doc.pages:\n",
    "    for table in page.tables:\n",
    "        row_text = str()\n",
    "        for r, row in enumerate(table.rows):            \n",
    "            for c, cell in enumerate(row.cells):\n",
    "                row_text = row_text + '\"' + cell.text + '\",'\n",
    "            row_text = row_text.strip(',')+\"\\n\"\n",
    "        all_tables.append(row_text)\n",
    "\n",
    "len(all_tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0621a36-aaf2-4489-92ef-1765f7c4ac4f",
   "metadata": {},
   "source": [
    "This document contains more than one table so we will use the first table to perform Q&A on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7e292d-cb04-4167-83f8-deb8fa4ff4e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(all_tables[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0a686b-32ef-4818-8550-f1b0e85fcd3f",
   "metadata": {},
   "source": [
    "There are two tables in this page, let's do Q&A on the first table. Note that we are going to use LangChain's `SelfQueryRetriever` which is helpful with Q&A with tables. However since we are using Flan-T5 HuggingFace hosted API, the input token limit is only 1024 tokens. This is not suffieicient to accomodate all the rows of our table. You can deploy this model on SageMaker JumpStart with a large instance type and get more token limits, or perhaps use a different larger model such as Anthropic. For our demonstration purposes we will choose only the first 3 rows for the table via the `docs = docs[:3]` line of code.\n",
    "\n",
    "We will now load the 3 row table into Chroma DB and try to perform Q&A with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa497e5-6043-4818-9ced-2da886113f4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.spacy_embeddings import SpacyEmbeddings\n",
    "import csv\n",
    "from io import StringIO\n",
    "\n",
    "f = StringIO(all_tables[0])\n",
    "reader = csv.reader(f)\n",
    "headers = next(reader)\n",
    "result = []\n",
    "\n",
    "def to_integer(value):\n",
    "    try:\n",
    "        return int(float(value.replace(',', '')))\n",
    "    except ValueError:\n",
    "        return \"\"\n",
    "\n",
    "for row in reader:\n",
    "    if len(row) == len(headers):\n",
    "        for i in [-1, -2, -3]:\n",
    "            row[i] = to_integer(row[i])\n",
    "        metadata = {headers[i].strip(): row[i] for i in range(len(headers))}\n",
    "        page_content = \",\".join(map(str, row))\n",
    "        tuple_entry = (page_content, metadata)\n",
    "        result.append(tuple_entry)\n",
    "        \n",
    "docs = list()\n",
    "for item in result:\n",
    "    docs.append(Document(page_content=item[0],metadata=item[1]))\n",
    "docs = docs[:-3]\n",
    "\n",
    "# create the open-source embedding function\n",
    "embedding_function = SpacyEmbeddings()\n",
    "\n",
    "vectorstore = Chroma.from_documents(docs, embedding_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e017a63b-5c6d-4103-8d6a-0fdff2777554",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# To delete the Chroma DB in-memory collection, un-comment and execute the line below\n",
    "# vectorstore.delete_collection()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a3fc05-b74b-49d8-80d2-4ebd64205fa9",
   "metadata": {},
   "source": [
    "We have loaded our table in question into the vector database. Next we will create an LLM object using LangChain supported `SageMakerEndpoint` class. This object will call the SageMaker endpoint with the Falcon model from within the LangChain chain for inference. The `ContentHandler` class will receive the prompt, and then format it in a way that the SageMaker endpoint expects, it will also receive the output from the LLM and return the generated text from the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc45f3a-cc4b-44df-a1d1-ecce423eacbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from langchain import SagemakerEndpoint\n",
    "from langchain.llms.sagemaker_endpoint import LLMContentHandler\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "\n",
    "class ContentHandler(LLMContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs: dict) -> bytes:        \n",
    "        prompt = re.sub(r'<< Example 2. >>.*?(?=<< Example 3. >>)', '', prompt, flags=re.DOTALL).replace('<< Example 3. >>','<< Example 2. >>') # we will shorten the Langchain injected prompt a little bit\n",
    "        input_str = json.dumps({\"inputs\": prompt,  \"parameters\": model_kwargs})         \n",
    "        return input_str.encode('utf-8')\n",
    "    \n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))        \n",
    "        return response_json[0][\"generated_text\"]\n",
    "\n",
    "content_handler = ContentHandler()\n",
    "\n",
    "llm=SagemakerEndpoint(\n",
    "        endpoint_name=endpoint_name, \n",
    "        region_name=region,\n",
    "        model_kwargs={\"do_sample\": True,\n",
    "                                    \"top_p\": 0.9,\n",
    "                                    \"temperature\": 0.8,\n",
    "                                    \"max_new_tokens\":  100,\n",
    "                                    \"stop\": [\"<|endoftext|>\", \"</s>\"]},\n",
    "        content_handler=content_handler\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80e27b8-1fa6-413f-bf57-ffcc49bb2473",
   "metadata": {},
   "source": [
    "In the final step, we define the schema of the table using LangChain `AttributeInfo` model which will help the LLM understand the structure of the table and subsequently create a retriever using the LLM (we created earlier), the vector store, and the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ef8f61-a794-4643-89f3-d07dd2dfd3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "metadata_field_info = [\n",
    "    AttributeInfo(\n",
    "        name=\"Date\",\n",
    "        description=\"Date of the bank transaction\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"Description\",\n",
    "        description=\"Description of the bank transaction\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"Deposits ($)\",\n",
    "        description=\"The dollar amount deposited into the bank account\",\n",
    "        type=\"integer\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"Withdrawals ($)\",\n",
    "        description=\"The dollar amount withdrawn from the bank account\",\n",
    "        type=\"integer\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"Amount ($)\",\n",
    "        description=\"The total dollar amount balance in the bank account\",\n",
    "        type=\"integer\",\n",
    "    )\n",
    "]\n",
    "document_content_description = \"Bank Statement\"\n",
    "\n",
    "retriever = SelfQueryRetriever.from_llm(\n",
    "    llm, vectorstore, document_content_description, metadata_field_info, \n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88970662-f5ae-48b1-adb9-c7c7cd580aad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    retriever.get_relevant_documents(\"List the transactions with more than $1000 in Deposits ($)\")\n",
    "except Exception as e:\n",
    "    print(str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f3b051-b3d2-47ad-86a4-a1d2bd01b88c",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "---\n",
    "\n",
    "Delete the SageMaker Jumpstart endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300c4338-3d92-4c78-a830-d00005972b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
