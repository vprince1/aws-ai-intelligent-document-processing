{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f6e38fc-9f24-4c8f-9e2e-2c120eb68d45",
   "metadata": {},
   "source": [
    "# Augment Intelligent Document Processing with generative AI using Amazon Bedrock\n",
    "---\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> \n",
    "    <b>NOTE:</b> You will need to use a Jupyter Kernel with Python 3.9 or above to use this notebook. If you are in Amazon SageMaker Studio, you can use the `Data Science 3.0` image.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"> \n",
    "    <b>NOTE:</b> You will need 3rd party model access to Anthropic Claude V1 model to be able to run this notebook. Verify if you have access to the model by going to <a href=\"https://console.aws.amazon.com/bedrock\" target=\"_blank\">Amazon Bedrock console</a> > left menu \"Model access\". The \"Access status\" for Anthropic Claude must be in \"Access granted\" status in green. If you do not have access, then click \"Edit\" button on the top right > select the model checkbox > click \"Save changes\" button at the bottom. You should have access to the model within a few moments.\n",
    "</div>\n",
    "\n",
    "In this notebook, we demonstrate how you can integrate Amazon Textract with Amazon Bedrock as a document loader to extract data from documents and use generative AI capabilities within the various IDP phases. We will perform the following with different LLMs.\n",
    "\n",
    "- Classification\n",
    "- Summarization\n",
    "- Standardization\n",
    "- Spell check corrections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3b0ee4-b856-4718-992e-99390f56068e",
   "metadata": {},
   "source": [
    "## Setup Prerequisites\n",
    "---\n",
    "\n",
    "First we need to \n",
    "1. Install Amazon Textractor. This open source python library makes it easy to parse and handle the JSON output from Amazon Textract\n",
    "1. Import script libraries and create global variables\n",
    "1. Create a global function for calling Amazon Bedrock \n",
    "1. Upload our example files to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c12c62-2259-48b9-8148-70a540c28ae5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python -m pip install amazon-textract-textractor[pdf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bd886e-d953-4b0e-81a1-54d72847b8f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import script libraries and create global variables\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import sagemaker\n",
    "import datetime\n",
    "import boto3\n",
    "from textractor import Textractor\n",
    "from textractor.data.constants import TextractFeatures\n",
    "from textractor.parsers import response_parser\n",
    "\n",
    "extractor = Textractor(region_name=\"us-east-2\")\n",
    "textract = boto3.client('textract')\n",
    "s3 = boto3.client(\"s3\")\n",
    "# in case of running this notebook in Workshop Studio environment, set region_name = us-west-2 for Bedrock models.\n",
    "bedrock = boto3.client(service_name=\"bedrock-runtime\", region_name=\"us-west-2\")\n",
    "role = sagemaker.get_execution_role()\n",
    "data_bucket = sagemaker.Session().default_bucket()\n",
    "print(f\"SageMaker bucket is {data_bucket}, and SageMaker Execution Role is {role}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bc12a4-44de-4352-84b9-3218813fe731",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a global function to call Bedrock. \n",
    "def get_response_from_claude(prompt):\n",
    "\t\"\"\"\n",
    "\tInvokes Anthropic Claude 3 Haiku to run a text inference using the input\n",
    "\tprovided in the request body.\n",
    "\n",
    "\t:param prompt:            The prompt that you want Claude 3 to use.\n",
    "\t:return: Inference response from the model.\n",
    "\t\"\"\"\n",
    "\n",
    "\t# Invoke the model with the prompt and the encoded image\n",
    "\tmodel_id = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "\trequest_body = {\n",
    "\t\t\"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "\t\t\"max_tokens\": 2048,\n",
    "        \"temperature\":0.5,\n",
    "\t\t\"messages\": [\n",
    "\t\t\t{\n",
    "\t\t\t\t\"role\": \"user\",\n",
    "\t\t\t\t\"content\": [\n",
    "\t\t\t\t\t{\n",
    "\t\t\t\t\t\t\"type\": \"text\",\n",
    "\t\t\t\t\t\t\"text\": prompt,\n",
    "\t\t\t\t\t},\n",
    "\t\t\t\t],\n",
    "\t\t\t}\n",
    "\t\t],\n",
    "\t}\n",
    "\n",
    "\ttry:\n",
    "\t\tresponse = bedrock.invoke_model(\n",
    "\t\t\tmodelId=model_id,\n",
    "\t\t\tbody=json.dumps(request_body),\n",
    "\t\t)\n",
    "\n",
    "\t\t# Process and print the response\n",
    "\t\tresult = json.loads(response.get(\"body\").read())\n",
    "\t\tinput_tokens = result[\"usage\"][\"input_tokens\"]\n",
    "\t\toutput_tokens = result[\"usage\"][\"output_tokens\"]\n",
    "\t\t# the current Bedrock Claude Messagees API only supports text content in responses\n",
    "\t\ttext_response = result[\"content\"][0][\"text\"]\n",
    "\n",
    "        # return a tuple with 3 values\n",
    "\t\treturn text_response, input_tokens, output_tokens\n",
    "\texcept ClientError as err:\n",
    "\t\tlogger.error(\n",
    "\t\t\t\"Couldn't invoke Claude 3 Sonnet. Here's why: %s: %s\",\n",
    "\t\t\terr.response[\"Error\"][\"Code\"],\n",
    "\t\t\terr.response[\"Error\"][\"Message\"],\n",
    "\t\t)\n",
    "\t\traise\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903c0117-ccbb-4985-aa68-5f2d23fe1022",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Upload the sample files to S3\n",
    "sample_files = [\n",
    "    'samples/discharge-summary.png',\n",
    "    'samples/hand_written_note.pdf',\n",
    "    'samples/health_plan.pdf',\n",
    "    ]\n",
    "for file_key in sample_files:\n",
    "    s3.upload_file(Filename='./'+file_key, Bucket=data_bucket, Key=file_key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1585e7-0776-4482-b89d-78f348504925",
   "metadata": {},
   "source": [
    "## 1. Classification\n",
    "---\n",
    "\n",
    "First we extract the text from the document into a format that is useful for RAG processing with our LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9093a6-ce0f-481e-8d82-14a07a860191",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Examples 1, 2, and 3 will use the discharge summary document. You can find the sample documents in the samples folder. Take a look at this documen before proceeding\n",
    "file_key = 'samples/discharge-summary.png'\n",
    "\n",
    "# We will use the Textract detect document text action to get all the text in the document.\n",
    "textract_response = textract.detect_document_text(\n",
    "    Document={'S3Object': \n",
    "              {'Bucket': data_bucket,'Name': file_key}\n",
    "             }, \n",
    "\t)\n",
    "\t\n",
    "# Textractor provides a parser to give us a summary of the contents and a string with the detected text\n",
    "document = response_parser.parse(textract_response)\n",
    "\n",
    "print(document)\n",
    "doc_text = document.get_text()\n",
    "print (f\"\\nDetected Text \\n=========================\\n\")\n",
    "print(doc_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed904147-9ed1-4db1-81ee-5a433624b204",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Claude can classify a document based on it's content, given a list of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5641327d-e375-4c03-ac1e-96704bcdbd89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "\n",
    "Given a list of classes and a document, classify the document into one of the given classes. \n",
    "\n",
    "<classes>DISCHARGE_SUMMARY, RECEIPT, PRESCRIPTION</classes>\n",
    "<document>{doc_text}<document>\n",
    "\n",
    "\n",
    "\n",
    "Skip any preamble text and just give the class name.\n",
    "\"\"\"\n",
    "\n",
    "response = get_response_from_claude(prompt)\n",
    "\n",
    "print(f\"The provided document is = {response[0]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac62d59-5a42-4546-b26a-641db1992cfc",
   "metadata": {},
   "source": [
    "## 2. Summarization\n",
    "---\n",
    "\n",
    "Summarize large pieces of text from a document into smaller, more coincise explanations. In this block we will ask Claude to summarize the discharge summary.\n",
    "\n",
    "Try editing the prompt to provide a suggested next step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4212bb6d-9069-4f34-a684-4d7d1750e6a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "\n",
    "Given a full document, give me a concise summary. \n",
    "\n",
    "<document>{doc_text}</document>\n",
    "\n",
    "Skip any preamble text and just give the summary.\n",
    "\n",
    "<summary>\"\"\"\n",
    "\n",
    "summary = get_response_from_claude(prompt)\n",
    "\n",
    "print (f\"Our prompt has {summary[1]} input tokens and Claude returned {summary[2]} output tokens \\n\\n=========================\\n\")\n",
    "print(summary[0].replace(\"</summary>\",\"\").strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564513a1-e9ab-4b53-a6ea-b506d80586ab",
   "metadata": {},
   "source": [
    "## 3. Standardization\n",
    "---\n",
    "\n",
    "Let's try to standardize dates from our discharge summary document. Note that the document has dates in `DD-MON-YYYY` format, and we want to convert all of those dates to `MM/DD/YYYY` format. We will use simple prompt engineering techniques to show Claude some example and have it generate the output in a JSON format (Key value pair)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a831d72-f6c8-4bff-916f-f560d1915e33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question = \"Can you give me the patient admitted and discharge dates?\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "\n",
    "Given a full document, answer the question and format the output in the format specified. \n",
    "\n",
    "<format>\n",
    "{{\n",
    "  \"key_name\":\"key_value\"\n",
    "}}\n",
    "</format>\n",
    "<document>{doc_text}</document>\n",
    "<question>{question}</question>\n",
    "\n",
    "<output_instructions>\n",
    "Skip any preamble text and just generate the JSON.\n",
    "Format the dates in the value fields precisely in this format <format>DD/MM/YYYY</format>. \n",
    "</output_instructions>\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "summary = get_response_from_claude(prompt)\n",
    "\n",
    "print (f\"Our prompt has {summary[1]} input tokens and Claude returned {summary[2]} output tokens \\n\\n=========================\\n\")\n",
    "print(summary[0].replace(\"</summary>\",\"\").strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b249e4-8028-4998-8d4a-7f72d7855b38",
   "metadata": {},
   "source": [
    "## 4. Spell check and corrections\n",
    "---\n",
    "\n",
    "Perform grammatical and spelling corrections on text extracted from a hand written document.\n",
    "\n",
    "In this example we provide Textract a handwritten note. We take the output of the note and send it to Claude to make corrections. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c244d3-7277-458a-afe5-5d7ec688890d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_key = 'samples/hand_written_note.pdf'\n",
    "\n",
    "textract_response = textract.detect_document_text(\n",
    "    Document={'S3Object': \n",
    "              {'Bucket': data_bucket,'Name': file_key}\n",
    "             }, \n",
    "\t)\n",
    "\t\n",
    "document = response_parser.parse(textract_response)\n",
    "\n",
    "print(document)\n",
    "doc_text = document.get_text()\n",
    "print (f\"\\nDetected Text \\n=========================\\n\")\n",
    "print(doc_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747a0598-d7ac-4513-b422-cf5bfc28d91a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "\n",
    "Given a detailed 'Document':\n",
    "\n",
    "<document>{doc_text}</document>\n",
    "\n",
    "perform spelling and grammatical corrections. Ensure the output is coherent, polished, and free from errors. \n",
    "\n",
    "Skip any preamble text and give the answer.\n",
    "<answer>\"\"\"\n",
    "\n",
    "answer = get_response_from_claude(prompt)\n",
    "print (f\"Our prompt has {answer[1]} input tokens and Claude returned {answer[2]} output tokens.\")\n",
    "print(\"\\nCorrected Text\")\n",
    "print(\"==============\")\n",
    "print(answer[0].replace(\"</answer>\",\"\").strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae4d20a-eef5-4147-ac82-647ee44ed467",
   "metadata": {},
   "source": [
    "## 5. Multi-page summarization\n",
    "\n",
    "We will now attempt to summarize a multi-page document.\n",
    "\n",
    "We will use the Textractor document loader to process a multi-page file store on S3. Textractor makes the asynchronous call to textract and handles the wait for response. \n",
    "\n",
    "Textractor will give us a page by page summary of the document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1291ee4-2688-458e-b845-efc093091b84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_key = 'samples/health_plan.pdf'\n",
    "file_s3_uri = f\"s3://{data_bucket}/{file_key}\"\n",
    "\n",
    "document = extractor.start_document_text_detection(file_source=file_s3_uri,\n",
    "# features=[TextractFeatures.LAYOUT],\n",
    "    save_image=False\n",
    ")\n",
    "document.pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb1a4e1-1c10-4ac3-be3c-4f68d82556b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"The document has {len(document.words)} total words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befeda3c-1608-4b4f-9625-c02771cfe202",
   "metadata": {},
   "source": [
    "Since Claude supports a 200k token context window we don't need to further split this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80098db4-e341-4a91-80b1-1f57b5830583",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "doc_text = document.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1a341b-3eeb-4512-b3e6-120b425941d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "\n",
    "Given a full document\n",
    "\n",
    "<document>{doc_text}</document>\n",
    "\n",
    "Give me a concise summary. \n",
    "\n",
    "Skip any preamble text and just give the summary.\n",
    "\n",
    "<summary>\"\"\"\n",
    "\n",
    "summary = get_response_from_claude(prompt)\n",
    "\n",
    "print (f\"Our prompt has {summary[1]} input tokens and Claude returned {summary[2]} output tokens \\n\\n=========================\\n\")\n",
    "print(summary[0].replace(\"</summary>\",\"\").strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497cb636-9ea2-4f3f-aa6e-a856574a2a17",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "---\n",
    "Let's delete the sample files we uploaded earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc2d322-c0b9-4eec-b544-edf6d761b15b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for file_key in sample_files:\n",
    "    s3.delete_object(Bucket=data_bucket, Key=file_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969fbd21-e126-4bde-9907-29d30ec0c9bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-2:429704687514:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
